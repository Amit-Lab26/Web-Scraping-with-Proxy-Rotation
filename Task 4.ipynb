{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e53a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import requests\n",
    "import queue\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b01325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_proxies_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read proxies from a text file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        proxies = [line.strip() for line in file]\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbde63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy(proxy):\n",
    "    \"\"\"\n",
    "    Check if a proxy is working by making a test request.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get('https://magicpin.in/Mumbai/Kurla/Restaurant/Fast-N%E2%80%99-Fresh/store/1557c6b/delivery/', proxies={'http': proxy, 'https': proxy}, timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0fc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_working_proxy(proxies):\n",
    "    \"\"\"\n",
    "    Get a random working proxy from the list of proxies.\n",
    "    \"\"\"\n",
    "    random.shuffle(proxies)\n",
    "    for proxy in proxies:\n",
    "        if check_proxy(proxy):\n",
    "            return proxy\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54494a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_data(url, output_csv, proxies):\n",
    "    # Set up Selenium WebDriver with a random proxy\n",
    "    proxy = get_random_working_proxy(proxies)\n",
    "    if not proxy:\n",
    "        print(\"No working proxy found.\")\n",
    "        return\n",
    "    # Specify the path for the CSV file\n",
    "    csv_file_path = 'scraped.csv'\n",
    "    \n",
    "    # Use Edge WebDriver\n",
    "    driver_path = 'C:/Users/Khushi/Documents/Cooltrack/edgedriver_win64/msedgedriver.exe'\n",
    "    options = webdriver.EdgeOptions()\n",
    "    #options.use_chromium = True\n",
    "    options.add_argument('--proxy-server={}'.format(proxy))\n",
    "    driver = webdriver.Edge(executable_path=driver_path, options=options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Find all <span> elements within <p> elements with class \"listedItemDetails\"\n",
    "        span_elements = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'p.listedItemDetails span'))\n",
    "        )\n",
    "\n",
    "        # Click on each <span> element\n",
    "        for span_element in span_elements:\n",
    "            span_element.click()\n",
    "\n",
    "        # Get the updated HTML content\n",
    "        updated_html = driver.page_source\n",
    "\n",
    "        # Parse the updated HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(updated_html, 'html.parser')\n",
    "\n",
    "        # Initialize a list to store the data\n",
    "        data_list = []\n",
    "\n",
    "        # For example, find all elements with class 'itemInfo'\n",
    "        item_infos = soup.find_all('article', class_='itemInfo')\n",
    "\n",
    "        for item_info in item_infos:\n",
    "            item_name = item_info.find('p', class_='itemName').a.text.strip()\n",
    "            item_price = item_info.find('span', class_='itemPrice').text.strip()\n",
    "\n",
    "            # Save item_name and item_price in the list\n",
    "            data_list.append({'Item Name': item_name, 'Item Price': item_price})\n",
    "\n",
    "        # Replace the currency symbol in the 'Item Price' field\n",
    "        for item in data_list:\n",
    "            if 'â‚¹' in item['Item Price']:\n",
    "                item['Item Price'] = 'Rs.' + item['Item Price'][1:]\n",
    "\n",
    "        # Save the list to a CSV file\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Item Name', 'Item Price']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write the header\n",
    "            writer.writeheader()\n",
    "\n",
    "            # Write the data\n",
    "            writer.writerows(data_list)\n",
    "\n",
    "        print(f'List saved to CSV file: {output_csv}')\n",
    "\n",
    "    finally:\n",
    "        # Close the browser window\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf986755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Specify the list of websites to scrape\n",
    "    websites = [\n",
    "    {'url': 'https://magicpin.in/Mumbai/Kurla/Restaurant/Fast-N%E2%80%99-Fresh/store/1557c6b/delivery/', 'output_csv': 'output1.csv'},\n",
    "    {'url': 'https://magicpin.in/Mumbai/Chembur/Restaurant/Rare-Kitchen-Friends-and-Family/store/1553893/delivery/', 'output_csv': 'output2.csv'},\n",
    "    {'url': 'https://magicpin.in/Mumbai/Govandi-West/Restaurant/Ss-Global-Chinese-Hut/store/1593116/delivery/', 'output_csv': 'output3.csv'},\n",
    "    # Add more websites as needed\n",
    "    ]\n",
    "\n",
    "    # Specify the path to the text file containing proxies\n",
    "    proxy_file_path = 'proxy_list.txt'\n",
    "    proxies = read_proxies_from_file(proxy_file_path)\n",
    "\n",
    "    # Scrape data for each website with a random proxy\n",
    "    for website in websites:\n",
    "        scrape_and_save_data(website['url'], website['output_csv'], proxies)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227734f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
